/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.hive

import java.io.File

import org.apache.spark.sql.Row
import org.apache.spark.sql.functions._
import org.apache.spark.sql.hive.HivemallOps._
import org.apache.spark.sql.hive.HivemallUtils._
import org.apache.spark.sql.types._
import org.apache.spark.test.HivemallQueryTest
import org.apache.spark.util.Utils

final class XGBoostSuite extends HivemallQueryTest {

  import hiveContext.implicits._

  test("train_xgboost_regr") {
    val trainDf = spark.read.format("libsvm").load(trainDir.getAbsolutePath)
    // Test data must be cached because of rowid()
    val testDf = spark.read.format("libsvm").load(testDir.getAbsolutePath)
      .withColumn("rowid", rowid()).cache

    val xgbOptions = XGBoostOptions()
      .set("num_round", "10")
      .set("max_depth", "4")

    var tempDir: File = null
    try {
      tempDir = Utils.createTempDir()

      val numModles = 1
      val modelDir = tempDir.getAbsolutePath + "/xgboost_model"

      trainDf.repartition(numModles)
        .train_xgboost_regr($"features", $"label", s"${xgbOptions}")
        .write.format(xgboost).save(modelDir)

      // Check #models generated by XGBoost
      assert(tempDir.listFiles().length == numModles)

      val model = hiveContext.sparkSession.read.format(xgboost).load(modelDir)
      val predict = model.join(testDf)
        .xgboost_predict($"rowid", $"features", $"model_id", $"pred_model")
        .groupBy("rowid").avg()
        .as("rowid", "predicted")
      val result = predict.join(testDf, predict("rowid") === testDf("rowid"), "INNER")
        .select(predict("rowid"), $"predicted", $"label")

      result.select(avg(abs($"predicted" - $"label"))).collect.map {
        case Row(diff: Double) => assert(diff > 0.0)
      }
    } catch {
      case e: Throwable => fail(s"Unexpected exception detected: ${e}")
    } finally {
      Utils.deleteRecursively(tempDir)
    }
  }

  test("train_xgboost_classifier") {
    val trainDf = spark.read.format("libsvm").load(trainDir.getAbsolutePath)
    // Test data must be cached because of rowid()
    val testDf = spark.read.format("libsvm").load(testDir.getAbsolutePath)
      .withColumn("rowid", rowid()).cache

    val xgbOptions = XGBoostOptions()
      .set("num_round", "10")
      .set("max_depth", "4")

    var tempDir: File = null
    try {
      tempDir = Utils.createTempDir()

      val numModles = 1
      val modelDir = tempDir.getAbsolutePath + "/xgboost_model"

      trainDf.repartition(numModles)
        .train_xgboost_regr($"features", $"label", s"${xgbOptions}")
        .write.format(xgboost).save(modelDir)

      // Check #models generated by XGBoost
      assert(tempDir.listFiles().length == numModles)

      val model = hiveContext.sparkSession.read.format(xgboost).load(modelDir)
      val predict = model.join(testDf)
        .xgboost_predict($"rowid", $"features", $"model_id", $"pred_model")
        .groupBy("rowid").avg()
        .as("rowid", "predicted")
      val result = predict.join(testDf, predict("rowid") === testDf("rowid"), "INNER")
        .select(
          when($"predicted" >= 0.50, 1).otherwise(0),
          $"label".cast(IntegerType)
        )
        .toDF("predicted", "label")

      assert((result.where($"label" === $"predicted").count + 0.0) / result.count > 0.0)
    } catch {
      case e: Throwable => fail(s"Unexpected exception detected: ${e}")
    } finally {
      Utils.deleteRecursively(tempDir)
    }
  }

  test("train_xgboost_multiclass_classifier") {
    val trainDf = spark.read.format("libsvm").load(trainDir.getAbsolutePath)
    // Test data must be cached because of rowid()
    val testDf = spark.read.format("libsvm").load(testDir.getAbsolutePath)
      .withColumn("rowid", rowid()).cache

    val xgbOptions = XGBoostOptions()
      .set("num_round", "10")
      .set("max_depth", "4")
      .set("num_class", "2")

    var tempDir: File = null
    try {
      tempDir = Utils.createTempDir()

      val numModles = 1
      val modelDir = tempDir.getAbsolutePath + "/xgboost_model"

      trainDf.repartition(numModles)
        .train_xgboost_multiclass_classifier($"features", $"label", s"${xgbOptions}")
        .write.format(xgboost).save(modelDir)

      // Check #models generated by XGBoost
      assert(tempDir.listFiles().length == numModles)

      val model = hiveContext.sparkSession.read.format(xgboost).load(modelDir)
      val predict = model.join(testDf)
        .xgboost_multiclass_predict($"rowid", $"features", $"model_id", $"pred_model")
        .groupby("rowid").max_label("probability", "label")
        .toDF("rowid", "predicted")
      val result = predict.join(testDf, predict("rowid") === testDf("rowid"), "INNER")
        .select(
          predict("rowid"),
          $"predicted",
          $"label".cast(IntegerType)
        )

      assert((result.where($"label" === $"predicted").count + 0.0) / result.count > 0.0)
    } catch {
      case e: Throwable => fail(s"Unexpected exception detected: ${e}")
    } finally {
      Utils.deleteRecursively(tempDir)
    }
  }
}
